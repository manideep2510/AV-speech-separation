Training data: 360
Validation data: 240
3D Conv Out: (None, 50, 32, 32, 64)
3D Conv Out Reshape: (None, 32, 32, 64)
Resnet18 Out: (None, 1, 1, 512)
Resnet18 Linear Out: (None, 256)
Input to GRU: (None, 50, 256)
GRU Out: (None, 50, 29)
ResNet LSTM Pretrain weights loaded
outv: (None, 200, 256)
outa: (None, 200, 256)
hidden (None, 256)
attn_out: (None, 200, 512)
attn_states: (None, 200, 200)
fusion: (None, 200, 512)

==================================================================================================
Total params: 29,658,113
Trainable params: 18,093,569
Non-trainable params: 11,564,544
__________________________________________________________________________________________________

Model weights path: test_tdavss_Offset_BothHigh_epochs40_5lr1e-4_exp1

Epoch 1/3
 1/23 [>.............................] - ETA: 3:22 - loss: 44.5889 - snr_acc: -44.5889Traceback (most recent call last):
  File "main_tasnet_unsync.py", line 162, in <module>
    callbacks=[reducelronplateau, checkpoint_save_weights, metrics_wandb, LoggingCallback(print_fcn=log_to_file), metrics_unsync], verbose=1)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py", line 1297, in fit_generator
    steps_name='steps_per_epoch')
  File "/usr/local/lib/python3.6/dist-packages/wandb/keras/__init__.py", line 104, in new_generator
    return old_generator(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py", line 265, in model_iteration
    batch_outs = batch_function(*batch_data)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py", line 973, in train_on_batch
    class_weight=class_weight, reset_metrics=reset_metrics)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py", line 264, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py", line 311, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_eager.py", line 268, in _process_single_batch
    grads = tape.gradient(scaled_total_loss, trainable_weights)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py", line 1014, in gradient
    unconnected_gradients=unconnected_gradients)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/imperative_grad.py", line 76, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/backprop.py", line 138, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_grad.py", line 247, in _MaxPool3DGrad
    data_format=op.get_attr("data_format").decode())
  File "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py", line 5889, in max_pool3d_grad
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File "<string>", line 3, in raise_from
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[16,64,50,33,66] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MaxPool3DGrad]
